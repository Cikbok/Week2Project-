# -*- coding: utf-8 -*-
"""Cheatsheet Classfication

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GGfZZsgz8i_P3Fi1K8EvEQCwJXZ4dECR
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 


import matplotlib.pyplot as plt

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

from scipy import stats

# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory

import seaborn as sns

import os

# Any results you write to the current directory are saved as output.

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import roc_curve, auc
from sklearn.preprocessing import label_binarize
from sklearn.linear_model import RidgeClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.gaussian_process.kernels import RBF
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier


from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import average_precision_score
from sklearn.linear_model import RidgeClassifierCV
from sklearn import preprocessing
from sklearn.feature_selection import RFE
from sklearn.preprocessing import LabelEncoder

pd.get_dummies(X,drop_first=True)

#check null data
train_data["age"].isnull().sum()/train_data.shape[0]
#No null data

#Check data distribution 
train_data["age"].hist()

#Check y distribution on independable variables
train_data.groupby("job").y.value_counts(normalize=True).unstack()[1].plot(kind="bar")

#print classfication metrics
def print_classfiction_metrics(testy,yhat_classes):
    # accuracy: (tp + tn) / (p + n)
    accuracy = accuracy_score(testy, yhat_classes)
    print('Accuracy: %f' % accuracy)
    # precision tp / (tp + fp)
    precision = precision_score(testy, yhat_classes)
    print('Precision: %f' % precision)
    # recall: tp / (tp + fn)
    recall = recall_score(testy, yhat_classes)
    print('Recall: %f' % recall)
    # f1: 2 tp / (2 tp + fp + fn)
    f1 = f1_score(testy, yhat_classes)
    print('F1 score: %f' % f1)

#Split test and train 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

#print confusion matrix 
base_clf = LogisticRegression(random_state=0).fit(X_train, y_train)
y_pred=base_clf.predict(X_test)
y_score = base_clf.fit(X_train, y_train).decision_function(X_test)
# Compute ROC curve and ROC area for each class
fpr, tpr, _ = roc_curve(y_test, y_score)
roc_auc = auc(fpr, tpr)
C=confusion_matrix(y_test,y_pred)
sns.heatmap(C / C.astype(np.float).sum(axis=1))
plt.title("Confusion Matrix Normalized")

## Changing Thresholds of the logistic model 
####################################
# The optimal cut off would be where tpr is high and fpr is low
# tpr - (1-fpr) is zero or near to zero is the optimal cut off point
####################################
i = np.arange(len(tpr)) # index for df
roc = pd.DataFrame({'fpr' : pd.Series(fpr, index=i),'tpr' : pd.Series(tpr, index = i), '1-fpr' : pd.Series(1-fpr, index = i), 'tf' : pd.Series(tpr - (1-fpr), index = i), 'thresholds' : pd.Series(_, index = i)})
roc.ix[(roc.tf-0).abs().argsort()[:1]]

# Plot tpr vs 1-fpr
fig, ax = plt.subplots()
plt.plot(roc['tpr'])
plt.plot(roc['1-fpr'], color = 'red')
plt.xlabel('1-False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic')
ax.set_xticklabels([])


#ROC curve with thresholds

def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):
    """
    Modified from:
    Hands-On Machine learning with Scikit-Learn
    and TensorFlow; p.89
    """
    plt.figure(figsize=(8, 8))
    plt.title("Precision and Recall Scores as a function of the decision threshold")
    plt.plot(thresholds, precisions[:-1], "b--", label="Precision")
    plt.plot(thresholds, recalls[:-1], "g-", label="Recall")
    plt.ylabel("Score")
    plt.xlabel("Decision Threshold")
    plt.legend(loc='best')

p, r, thresholds = precision_recall_curve(y_test, y_score)
plot_precision_recall_vs_threshold(p,r,thresholds)
#precision and recall with threshold

#Find the best threshold using accuracy 
def find_best_db(y_test,y_proba):
    threds=np.linspace(0.45,0.55,100)
    accrs={}
    for thre in threds:
        new_pred=[1 if i>thre else 0 for i in y_proba]
        accrs[thre]=accuracy_score(y_test,new_pred)
    sorted_x = sorted(accrs.items(), key=lambda x: x[1],reverse=True)
    return sorted_x   
  find_best_db(y_test,clf.predict_proba(X_test)[:,1])[:5]

print("Area Under ROC Curve:", roc_auc)

#grid search example 
params = {'penalty':['l1','l2'],'C':[0.01,0.1,1,10],'solver':['liblinear','saga']}
# Create grid search using 5-fold cross validation
best_lg = GridSearchCV(LogisticRegression(max_iter=100), params, cv=5, verbose=0,scoring='accuracy',return_train_score=True)
best_lg.fit(X_train,y_train)

#scale numerical variables 
numerics=["age","campaign","previous","emp.var.rate","cons.price.idx","cons.conf.idx","euribor3m","nr.employed"]
X[numerics] = preprocessing.scale(X[numerics])

#all classfication models and check AUC 
names = ["Nearest Neighbors", 
         "Decision Tree", "Random Forest", "Neural Net", "AdaBoost",
         "Naive Bayes", "QDA"]

classifiers = [
    KNeighborsClassifier(3),
    DecisionTreeClassifier(max_depth=5),
    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
    MLPClassifier(alpha=1, max_iter=1000),
    AdaBoostClassifier(),
    GaussianNB(),
    QuadraticDiscriminantAnalysis()]


    # iterate over classifiers
for name, clf in zip(names, classifiers):
    clf.fit(X_train, y_train.ravel())
    if hasattr(clf, "decision_function"):
        y_score = clf.decision_function(X_test)
    else:
        y_score = clf.predict_proba(X_test)[:, 1]
    y_pred=clf.predict(X_test)
    fpr, tpr, _ = roc_curve(y_test.ravel(), y_score)
    roc_auc = auc(fpr, tpr)
    print_classfiction_metrics(y_test.ravel(),y_pred)
    plt.plot(fpr, tpr,lw=2, label='ROC curve (area = {}) for {}'.format(roc_auc,name) )
  
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic Curve')
plt.legend(loc="upper left", bbox_to_anchor=(1.05,1.05))
plt.show()

